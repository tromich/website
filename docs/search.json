{
  "articles": [
    {
      "path": "about.html",
      "title": "About me",
      "description": "I am a Geography Ph.D. student at the University of California, Santa Barbara. Currently, I am researching the impacts of climate on conifer needle shedding, using the RHESSys ecohydrological model. I completed my Master's degree in Geography at UC Santa Barbara in 2020, after finishing a study on the impact of solar radiation on decomposition of common California grass species. Prior to attending graduate school, I worked seasonal jobs related to natural resources management for several years, in a variety of ecosystems. I completed my undergraduate degree in Science of Earth Systems at Cornell University in 2013.\n\nIn my free time I enjoy hiking, a passion that dates to childhood and that has inspired me to work in natural resources and conservation. In recent years I have made several trips to Death Valley with my family.\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:02-07:00"
    },
    {
      "path": "blog.html",
      "title": "Blog",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:02-07:00"
    },
    {
      "path": "index.html",
      "title": "Trevor Romich",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          website\r\n          \r\n          \r\n          Home\r\n          About Me\r\n          Project Portfolio\r\n          Resume\r\n          Blog\r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Trevor Romich\r\n            \r\n            \r\n              \r\n                \r\n                    \r\n                      \r\n                        GitHub\r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                        Email\r\n                      \r\n                    \r\n                  \r\n                                  \r\n            \r\n          \r\n        \r\n        \r\n        \r\n          \r\n            I am a Ph.D. student in Geography at the University of California, Santa Barbara, researching the impact of drought on conifer needle shedding, and the relationship this has with climate.\r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Trevor Romich\r\n            \r\n            \r\n              \r\n                \r\n                                    \r\n                    \r\n                      GitHub\r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      Email\r\n                    \r\n                  \r\n                                  \r\n              \r\n            \r\n            \r\n              I am a Ph.D. student in Geography at the University of California, Santa Barbara, researching the impact of drought on conifer needle shedding, and the relationship this has with climate.\r\n            \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2022-03-14T15:48:02-07:00"
    },
    {
      "path": "photos.html",
      "title": "Death Valley Photos",
      "description": "Photos I've taken from Death Valley. Currently under construction!\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:03-07:00"
    },
    {
      "path": "Portfolio.html",
      "title": "Project Portfolio",
      "author": [],
      "contents": "\r\n\r\n\r\n.a{ display : flex}\r\n\r\n\r\nI have worked with several different programming languages as part of research and coursework. The links below provide several recent examples of my data analysis projects.\r\n\r\n\r\nOxygen Saturation Model Selection\r\n\r\n\r\nTextual analysis - Thoreau Society bulletin\r\n\r\n\r\nNonlinear Models: Lizard Traits\r\n\r\n\r\nVisualization of Fish Ladder Observation Data\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:03-07:00"
    },
    {
      "path": "project1.html",
      "title": "Developing a model for oxygen saturation",
      "author": [],
      "contents": "\r\n\r\n\r\nhide\r\n\r\nlibrary(AICcmodavg)\r\nlibrary(dplyr)\r\nlibrary(equatiomatic)\r\nlibrary(knitr)\r\n\r\n\r\n\r\nFor this project, the goal was to test different linear models made using the data available at: https://calcofi.org/ccdata.html I downloaded this data on 1/30/2022.\r\nThe models I developed were as follows:\r\nOxygen saturation as a function of water temperature, salinity, and phosphate concentration\r\nOxygen saturation as a function of the three above factors and depth\r\nI will select the better model using AIC, then perform a ten-fold cross-validation on the two models using RMSE (root mean square error).\r\n\r\n\r\nhide\r\n\r\nt2data = read.csv(\"calcofi_seawater_samples.csv\")\r\n\r\n\r\n\r\nFirst, we generate the two models.\r\n\r\n\r\nhide\r\n\r\nmodel1 = lm(o2sat~t_deg_c+salinity+po4u_m,t2data)\r\nmodel2 = lm(o2sat~t_deg_c+salinity+po4u_m+depth_m,t2data)\r\n\r\n\r\n\r\nWe can obtain AIC (Akaiike’s Information Criteria) and corrected AIC values for each model using the functions in AICcmodavg. These values are intended to help select the preferred model.\r\n\r\n\r\nhide\r\n\r\naictable = AIC(model1,model2)\r\naicctable = data.frame(matrix(nrow=1,ncol=2))\r\naicctable[1,1]=AICc(model1)\r\naicctable[1,2]=AICc(model2)\r\ncolnames(aictable)=c(\"Degrees of Freedom\",\"AIC\")\r\nrownames(aictable)=c(\"Model 1\",\"Model 2\")\r\ncolnames(aicctable)=c(\"Model 1\",\"Model 2\")\r\nrownames(aicctable)=\"Corrected AIC\"\r\nkable(aictable)\r\n\r\n\r\n\r\nDegrees of Freedom\r\nAIC\r\nModel 1\r\n5\r\n618.3868\r\nModel 2\r\n6\r\n615.7016\r\n\r\nhide\r\n\r\nkable(aicctable)\r\n\r\n\r\n\r\nModel 1\r\nModel 2\r\nCorrected AIC\r\n619.0251\r\n616.6048\r\n\r\nThe AIC of the 2nd model is lower, and the difference is greater than two points, so it is preferred. This is true both for the regular and corrected AIC.\r\nNext, we perform the tenfold cross-validation.\r\n\r\n\r\nhide\r\n\r\n#rmse function\r\nrmse = function(x,y){\r\n  return(sqrt(mean((x-y)^2)))\r\n}\r\n\r\n\r\n#assign groups to data\r\ni = rep(1:10, length.out=nrow(t2data))\r\nset.seed(10)\r\nt2grdata = t2data %>%\r\n  mutate(group = sample(i, size=n(),replace=FALSE))\r\n\r\n#create output data frame\r\nrmse_output = data.frame(matrix(nrow=0,ncol=2))\r\ncolnames(rmse_output)=c(\"model 1\",\"model 2\")\r\n\r\n#iterate 10 times\r\nfor (i in 1:10){\r\n  train = t2grdata %>%\r\n    filter(group != i)\r\n  test = t2grdata %>%\r\n    filter(group == i)\r\n  \r\n  #generate each model\r\n  testmodel1 = lm(o2sat~t_deg_c+salinity+po4u_m,train)\r\n  testmodel2 = lm(o2sat~t_deg_c+salinity+po4u_m+depth_m,train)\r\n  \r\n  #predict the test data using each model\r\n  predict_model1 = predict(testmodel1,test)\r\n  predict_model2 = predict(testmodel2,test)\r\n  \r\n  #get rmse of each model\r\n  rmse_model1 = rmse(test$o2sat,predict_model1)\r\n  rmse_model2 = rmse(test$o2sat,predict_model2)\r\n  \r\n  #store these\r\n  rmse_output[i,1]=rmse_model1\r\n  rmse_output[i,2]=rmse_model2\r\n}\r\n\r\ntbl1 = rmse_output %>%\r\n  summarize(`Mean Model 1`=mean(rmse_output[,1]),`Mean Model 2` = mean(rmse_output[,2]))\r\n\r\n\r\n\r\nModel 2 has the lower RMSE, although the difference is very small. I would ordinarily be skeptical of overfitting given this, but the AIC value for model 2 is also lower, and AIC includes an attempt to account for overfitting. So I am inclined to favor model 2.\r\n\r\n\r\nhide\r\n\r\nmodel_final = lm(o2sat~t_deg_c+salinity+po4u_m+depth_m,t2data)\r\nsummary(model_final)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = o2sat ~ t_deg_c + salinity + po4u_m + depth_m, data = t2data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-26.3023  -2.2828  -0.2479   2.1771  19.4459 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 144.06686   95.36730   1.511   0.1342    \r\nt_deg_c      -0.74981    0.40494  -1.852   0.0672 .  \r\nsalinity     -0.43945    2.98897  -0.147   0.8834    \r\npo4u_m      -37.71159    2.50113 -15.078   <2e-16 ***\r\ndepth_m      -0.03196    0.01497  -2.135   0.0354 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 5.08 on 95 degrees of freedom\r\nMultiple R-squared:  0.9574,    Adjusted R-squared:  0.9557 \r\nF-statistic: 534.3 on 4 and 95 DF,  p-value: < 2.2e-16\r\n\r\nhide\r\n\r\nextract_eq(model_final,use_coefs = TRUE)\r\n\r\n\r\n\\[\r\n\\operatorname{\\widehat{o2sat}} = 144.07 - 0.75(\\operatorname{t\\_deg\\_c}) - 0.44(\\operatorname{salinity}) - 37.71(\\operatorname{po4u\\_m}) - 0.03(\\operatorname{depth\\_m})\r\n\\]\r\n\r\nAbove is the equation for Model 2, our preferred model which includes all four variables.\r\nInterestingly, salinity (which I didn’t exclude from either model the first time around) is not shown as having even a marginally significant impact on the prediction. So let’s try another model that excludes salinity.\r\n\r\n\r\nhide\r\n\r\n#here we are simply redoing the steps we did for model 1 and 2\r\n\r\n#generate the model\r\nmodel3 = lm(o2sat~t_deg_c+po4u_m+depth_m,t2data)\r\nsummary(model3)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = o2sat ~ t_deg_c + po4u_m + depth_m, data = t2data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-26.326  -2.363  -0.237   2.232  19.284 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 130.07069    5.65888  22.985   <2e-16 ***\r\nt_deg_c      -0.78428    0.32847  -2.388   0.0189 *  \r\npo4u_m      -37.96412    1.80878 -20.989   <2e-16 ***\r\ndepth_m      -0.03254    0.01436  -2.266   0.0257 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 5.054 on 96 degrees of freedom\r\nMultiple R-squared:  0.9574,    Adjusted R-squared:  0.9561 \r\nF-statistic: 719.8 on 3 and 96 DF,  p-value: < 2.2e-16\r\n\r\nhide\r\n\r\naictable = AIC(model1,model2,model3)\r\naicctable = data.frame(matrix(nrow=1,ncol=3))\r\naicctable[1,1]=AICc(model1)\r\naicctable[1,2]=AICc(model2)\r\naicctable[1,3]=AICc(model3)\r\ncolnames(aictable)=c(\"Degrees of Freedom\",\"AIC\")\r\nrownames(aictable)=c(\"Model 1\",\"Model 2\",\"Model 3\")\r\ncolnames(aicctable)=c(\"Model 1\",\"Model 2\",\"Model 3\")\r\nrownames(aicctable)=\"Corrected AIC\"\r\nkable(aictable)\r\n\r\n\r\n\r\nDegrees of Freedom\r\nAIC\r\nModel 1\r\n5\r\n618.3868\r\nModel 2\r\n6\r\n615.7016\r\nModel 3\r\n5\r\n613.7244\r\n\r\nhide\r\n\r\nkable(aicctable)\r\n\r\n\r\n\r\nModel 1\r\nModel 2\r\nModel 3\r\nCorrected AIC\r\n619.0251\r\n616.6048\r\n614.3627\r\n\r\nModel 3 does indeed have a lower AIC and corrected AIC than Model 2. The difference is just barely less than two points for AIC, but greater than two points for corrected AIC.\r\n\r\n\r\nhide\r\n\r\n#assign groups to data\r\ni = rep(1:10, length.out=nrow(t2data))\r\nset.seed(10)\r\nt2grdata = t2data %>%\r\n  mutate(group = sample(i, size=n(),replace=FALSE))\r\n\r\n#create output data frame\r\nrmse_output = data.frame(matrix(nrow=0,ncol=2))\r\ncolnames(rmse_output)=c(\"model 1\",\"model 2\")\r\n\r\n#iterate 10 times\r\nfor (i in 1:10){\r\n  train = t2grdata %>%\r\n    filter(group != i)\r\n  test = t2grdata %>%\r\n    filter(group == i)\r\n  \r\n  #generate each model\r\n  testmodel1 = lm(o2sat~t_deg_c+salinity+po4u_m,train)\r\n  testmodel2 = lm(o2sat~t_deg_c+salinity+po4u_m+depth_m,train)\r\n  testmodel3 = lm(o2sat~t_deg_c+po4u_m+depth_m,train)\r\n  \r\n  #predict the test data using each model\r\n  predict_model1 = predict(testmodel1,test)\r\n  predict_model2 = predict(testmodel2,test)\r\n  predict_model3 = predict(testmodel3,test)\r\n  \r\n  #get rmse of each model\r\n  rmse_model1 = rmse(test$o2sat,predict_model1)\r\n  rmse_model2 = rmse(test$o2sat,predict_model2)\r\n  rmse_model3 = rmse(test$o2sat,predict_model3)\r\n  \r\n  #store these\r\n  rmse_output[i,1]=rmse_model1\r\n  rmse_output[i,2]=rmse_model2\r\n  rmse_output[i,3]=rmse_model3\r\n}\r\n\r\ntbl2 = rmse_output %>%\r\n  summarize(`Mean Model 1`=mean(rmse_output[,1]),`Mean Model 2` = mean(rmse_output[,2]),`Mean Model 3` = mean(rmse_output[,3]))\r\nkable(tbl2)\r\n\r\n\r\nMean Model 1\r\nMean Model 2\r\nMean Model 3\r\n4.741503\r\n4.660267\r\n4.608762\r\n\r\nModel 3 has the lowest RMSE, but the difference is even less than the difference between Model 1 and Model 2.\r\nLet’s review all the information we have:\r\nModel 2 includes a term, salinity, which does not have even close to a marginally significant (i.e. at 90% confidence) impact on the predicted values in the model.\r\nModel 3 has a lower AIC than Model 2, which excludes salinity. However the difference between the models is less than 2 points, so we cannot conclude that Model 3 is superior to Model 2 based on this.\r\nModel 3 has a lower corrected AIC than Model 2, and by over two points. However, corrected AIC is intended to address problems with data where the sample size is very small, smaller than our available data.\r\nModel 3 has a lower RMSE than Model 2, but by an extremely small amount.\r\nOverall, Models 2 and 3 seem very close in performance, with mixed results suggesting that Model 3 may be the preferred model. In this scenario, I think the best option is to prefer the model that includes fewer terms - Model 3 - especially considering that the extra term in Model 2 does not appear to have a significant impact on Model 2’s predictions.\r\nTherefore, our new finalized model becomes:\r\n\r\n\r\nhide\r\n\r\nmodel_final = lm(o2sat~t_deg_c+po4u_m+depth_m,t2data)\r\nsummary(model_final)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = o2sat ~ t_deg_c + po4u_m + depth_m, data = t2data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-26.326  -2.363  -0.237   2.232  19.284 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 130.07069    5.65888  22.985   <2e-16 ***\r\nt_deg_c      -0.78428    0.32847  -2.388   0.0189 *  \r\npo4u_m      -37.96412    1.80878 -20.989   <2e-16 ***\r\ndepth_m      -0.03254    0.01436  -2.266   0.0257 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 5.054 on 96 degrees of freedom\r\nMultiple R-squared:  0.9574,    Adjusted R-squared:  0.9561 \r\nF-statistic: 719.8 on 3 and 96 DF,  p-value: < 2.2e-16\r\n\r\nhide\r\n\r\nextract_eq(model_final,use_coefs = TRUE)\r\n\r\n\r\n\\[\r\n\\operatorname{\\widehat{o2sat}} = 130.07 - 0.78(\\operatorname{t\\_deg\\_c}) - 37.96(\\operatorname{po4u\\_m}) - 0.03(\\operatorname{depth\\_m})\r\n\\]\r\n\r\nAll of the terms in this model are significant to at least 95% confidence.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:12-07:00"
    },
    {
      "path": "project2.html",
      "title": "Textual Analysis - Thoreau Society bulletin #122",
      "author": [],
      "contents": "\r\n\r\n\r\nhide\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(textdata)\r\nlibrary(pdftools)\r\nlibrary(ggwordcloud)\r\nlibrary(patchwork)\r\nlibrary(knitr)\r\n\r\n\r\n\r\nObjective: Textual analysis of Thoreau Society Bulletin #122.\r\nData source: Thoreau Society Bulletin One Hundred Twenty-Two. 1973. The Thoreau Society. Accessed 03/12/2022 via the Internet Archive at https://archive.org/details/thoreausociety1973122unse/mode/2up\r\nThis is an 8-page bulletin from the 1970s. We will analyze the bulletin for (1) word count, both total and by page and (2) sentiment, also by page and for the document as a whole.\r\n\r\n\r\nhide\r\n\r\ntxt_2_analyze = pdf_text('thoreausociety1973122unse.pdf') #load from pdf\r\ntxtdf = mutate(data.frame(txt_2_analyze),page=1:n()) #split by page\r\ntxtdf = mutate(txtdf,txt=str_split(txt_2_analyze,pattern='\\\\n')) %>% #split by line\r\n  unnest(txt) #lines are all subsets of a page, fix this [for some reason doing it without the '%>%' style does not work]\r\ntxtdf$txt = str_trim(txtdf$txt)\r\n\r\n#note that because of the format of this document (2 columns of text per page) the uploaded strings are, for the most part\r\n#fragments of two separate sentences - one from the left column and one from the right\r\n\r\n\r\n#get just the words\r\ntxt_words = txtdf %>%\r\n  unnest_tokens(word,txt) %>%\r\n  select(-txt_2_analyze)\r\n\r\n#let's do word count as well, like in lab\r\nwordct = arrange(count(txt_words,txt_words$word),desc(n))\r\n#unsurprisingly we have a lot of \"the,\" \"a,\" \"is,\" etc.\r\n\r\n\r\n\r\nAfter loading the data, we get rid of relatively unimportant words such as “the,” “a,” “is,” and so on.\r\n\r\n\r\nhide\r\n\r\n#Get rid of stop words\r\ntxtwords_stop = anti_join(txt_words,stop_words,by='word')\r\n#also get word count since we'll be working with it\r\nwordct_bypage = count(txtwords_stop,txtwords_stop$page,txtwords_stop$word) #by page\r\ncolnames(wordct_bypage)=c(\"page\",\"word\",\"n\")\r\nwordct_total = count(txtwords_stop,txtwords_stop$word) #overall\r\ncolnames(wordct_total)=c(\"word\",\"n\")\r\n\r\n\r\n\r\nBecause of the nature of this document, we are actually left with a fair number of ‘strange’ words, which we’ll see in a moment. However, as we will also see, it’s not necessarily a bad thing that these unusual words were left in, because they are informative.\r\nAfter loading the data and working it into a manageable form, our first objective is to determine the most frequently used words in the entire document. This excludes “stop words” (such as “the” or “is”) that are not particularly informative about the document’s contents. We’ll display this information using both bar charts for the top 10 words and a word cloud for the top 100.\r\n\r\n\r\nhide\r\n\r\n#now that we've gotten rid of pesky stop words, visualize the word count for words that matter\r\n\r\n#let's start with the whole document\r\n\r\n#get the total number of words:\r\ntotalwords = sum(wordct_total$n)\r\n\r\n#get the top 10 words\r\ntop10words = wordct_total%>%\r\n  arrange(-n)%>% #arrange by n in descending order\r\n  slice(1:10)#grab just the first 10 rows\r\n\r\n#plot this\r\ntop10_aggregate = ggplot(data=top10words,mapping=aes(x=n,y=reorder(word,n)))+geom_col()+theme_minimal()+labs(y=\"Word\",x=\"Number of uses\")\r\ntop10_aggregate+plot_annotation(caption=str_c(\"Figure 1: Usage frequency for the 10 most commonly used words in the entire document (\",as.character(totalwords),\" words in total),\\nThoreau Society Bulletin #122. Data (c) 1973, The Thoreau Society.\"))\r\n\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\n#now let's make a word cloud for the whole document\r\ntop100words_aggregate = slice(arrange(wordct_total,-n),1:100)\r\nwordcloud_aggregate = ggplot(top100words_aggregate,aes(label=word))+geom_text_wordcloud(aes(color=n,size=n),shape=\"circle\")+scale_size_area(max_size=12)+theme_minimal()+scale_color_gradientn(colors=c(\"#D8D8D8\",\"#000000\")) #make the words become more visible the more common they are, but lowest visibility should still be dark enough to read\r\nwordcloud_aggregate+plot_annotation(caption=str_c(\"Figure 2: Word cloud for the most commonly used words in the entire document (\",as.character(totalwords),\" words in total),\\nThoreau Society Bulletin #122. Larger and darker text indicates more commonly used words.\\nData (c) 1973, The Thoreau Society.\"))\r\n\r\n\r\n\r\n\r\nUnsurprisingly, “thoreau” is the most commonly used word in the entire document, and “walden” (a famous place in Thoreau’s life) is also extremely common. However we also see some odd words appearing quite frequently. I had to look in the PDF myself to figure out what “st” and “coll” meant; apparently they are abbreviations for “state college” and are used extensively in the membership list, since it is by state. “univ” likewise occurs often in this section. We should see this reflected in a difference in usage count by page, which is presented below.\r\n\r\n\r\nhide\r\n\r\n#easily get a data frame with just the pages by using count\r\npages = count(wordct_bypage,page)\r\n\r\n\r\n#get total words on each page\r\ntw_bypage = summarise(group_by(wordct_bypage,page),totalwords=sum(n))\r\ncolnames(tw_bypage)=c(\"Page #\", \"Total Words\")\r\nkable(tw_bypage,caption=\"Table 1: Word count by page\")\r\n\r\n\r\n(#tab:word count by page)Table 1: Word count by page\r\nPage #\r\nTotal Words\r\n1\r\n438\r\n2\r\n455\r\n3\r\n649\r\n4\r\n714\r\n5\r\n794\r\n6\r\n818\r\n7\r\n818\r\n8\r\n516\r\n\r\nAs can be seen in the table above, the total word count per page increases noticeably on pages 4-7, before declining again on page 8. This will be relevant later.\r\n\r\n\r\nhide\r\n\r\ntop10s = list()\r\nt10g = list()\r\n#for every page\r\nfor (i in 1:dim(pages)[1]){\r\n  current_page=pages$page[i]\r\n  \r\n  #get the top 10 words\r\n  top10_bypage = wordct_bypage%>%\r\n    filter(page==current_page)%>% #only for the current page\r\n    arrange(-n)%>% #arrange by n in descending order\r\n    slice(1:10)#grab just the first 10 rows\r\n  \r\n  top10s[i]=top10_bypage #store it\r\n  \r\n  #make our graph\r\n  top10_bpgraph = ggplot(data=top10_bypage,mapping=aes(x=n,y=reorder(word,n)))+geom_col()+theme_minimal()+labs(y=\"Word\",x=\"Number of uses\",title=str_c(\"Page \",as.character(i)))\r\n  t10g[[i]]= top10_bpgraph #double brackets appear to be needed to make it store the entire graph rather than just the data used to produce it\r\n}\r\n\r\n(t10g[[1]]+t10g[[2]])/(t10g[[3]]+t10g[[4]])+plot_annotation(caption=\"Figure 3: Top 10 words by page, for pages 1-4 in the document, Thoreau Society Bulletin #122.\\nData (c) 1973, The Thoreau Society. \")\r\n\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\n(t10g[[5]]+t10g[[6]])/(t10g[[7]]+t10g[[8]])+plot_annotation(caption=\"Figure 4: Top 10 words by page, for pages 5-8 in the document, Thoreau Society Bulletin #122.\\nData (c) 1973, The Thoreau Society.\")\r\n\r\n\r\n\r\n\r\nFrom examining these two sets of plots, we can see the document’s shift in focus to the membership list occurs on pages 4-7, where “st,” “coll,” and “univ” become relatively common at the expense of “thoreau” and “walden.”\r\nNext, we perform sentiment analysis on the entire document. For this we use the nrc lexicon. Mohammad, Saif M. and Turney, Peter D. (2013), Crowdsourcing a Word-Emotion Association Lexicon. Computational Intelligence, 29(3): 436-465. doi: 10.1111/j.1467-8640.2012.00460.x\r\n\r\n\r\nhide\r\n\r\n#attach the sentiment values for each word\r\nwords_nrcsentiment = inner_join(txtwords_stop,get_sentiments(\"nrc\"))\r\n#from viewing this after running it, I immediately notice an issue with the sentiment analysis: the word \"vice\" has 'negative' sentiment, even though in this document it is a component of the two-word title \"vice president.\" It only appears once so it shouldn't impact the results much\r\n\r\n#get the count of each sentiment type for the whole document\r\nwnrc_agg = count(words_nrcsentiment,sentiment)\r\n#plot this\r\nsentplot_agg = ggplot(wnrc_agg,aes(x=n,y=reorder(sentiment,n)))+geom_col()+theme_minimal()+labs(y=\"Sentiment\",x=\"Number of words\")\r\nsentplot_agg+plot_annotation(caption=\"Figure 5: Sentiment analysis using nrc lexicon for words in the entire document,\\nThoreau Society Bulletin #122. Text (c) 1973, The Thoreau Society.\\nnrc lexicon (c) 2013 Mohammad, Saif M. and Turney, Peter D.\")\r\n\r\n\r\n\r\n\r\nOverall, the sentiment of the document is positive and of trust, although there are a fair number of negative sentiment words as well.\r\nFinally, we look at sentiment of the document by page, again using nrc lexicon.\r\n\r\n\r\nhide\r\n\r\n#get the count of sentiments by page\r\n\r\nsents_bypage = count(words_nrcsentiment,page,sentiment)\r\n\r\n#make graphs for each page\r\nsentg = list()\r\n#for every page\r\nfor (i in 1:dim(pages)[1]){\r\n  current_page=pages$page[i]\r\n    \r\n  #get the top 10 words\r\n  sents_temp = sents_bypage%>%\r\n    filter(page==current_page)%>% #only for the current page\r\n    arrange(-n) #arrange by n in descending order\r\n    \r\n  \r\n  #make our graph\r\n  sent_bpgraph = ggplot(data=sents_temp,mapping=aes(x=n,y=reorder(sentiment,n)))+geom_col()+theme_minimal()+labs(y=\"Sentiment\",x=\"Number of words\",title=str_c(\"Page \",as.character(i)))\r\n  sentg[[i]]= sent_bpgraph #double brackets appear to be needed to make it store the entire graph rather than just the data used to produce it\r\n}\r\n\r\n\r\n#display them\r\n(sentg[[1]]+sentg[[2]])/(sentg[[3]]+sentg[[4]])+plot_annotation(caption=\"Figure 6: Sentiment analysis by page using nrc lexicon for words in pages 1-4,\\nThoreau Society Bulletin #122. Text (c) 1973, The Thoreau Society.\\nnrc lexicon (c) 2013 Mohammad, Saif M. and Turney, Peter D.\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\n(sentg[[5]]+sentg[[6]])/(sentg[[7]]+sentg[[8]])+plot_annotation(caption=\"Figure 7: Sentiment analysis by page using nrc lexicon for words in pages 5-8,\\nThoreau Society Bulletin #122. Text (c) 1973, The Thoreau Society.\\nnrc lexicon (c) 2013 Mohammad, Saif M. and Turney, Peter D.\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\nggplot(sents_bypage,aes(page,n,group=sentiment))+geom_line(aes(color=sentiment),size=1.2)+\r\n  theme_minimal()+labs(x=\"Page number\",y=\"Number of words\")+\r\n  #we'll color similar sentiments with similar colors:\r\n  scale_colour_manual(values = c(\"#0000FF\",\"#FF0000\",\"0077FF\",\"#7200FF\",\"#007727\",\r\n                        \"#000499\",\"#777777\",\"#7200FF\",\"#995A5A\",\"#000000\"))+\r\nplot_annotation(caption=\"Figure 8: Sentiment analysis for each page using nrc lexicon,\\nThoreau Society Bulletin #122. Similar colors indicate similar sentiments.\\n Text (c) 1973, The Thoreau Society.\\nnrc lexicon (c) 2013 Mohammad, Saif M. and Turney, Peter D.\")\r\n\r\n\r\n\r\n\r\nWe can see from the by-page sentiment analysis that there are fewer categorized words on pages 5-7, the pages which are primarily taken up by the membership list. This is in spite of the fact that these pages have more total words than the others (see Table 1 above). This suggests that the membership list - which one might expect to be improperly categorized by sentiment analysis, when it does not really reflect any sentiment at all - has a relatively small impact on the calculated sentiment of the document as a whole. In other words, it is mainly (or at least disproportionately) the articles in the bulletin that gives it ‘positive’ and ‘trust’ sentiments.\r\nThis document was only 8 pages long, and thus I was able to easily manually check the document as I was coding this. However, this process could also be done for a longer document, and the results would clue us in on locations to manually check, saving time.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:23-07:00"
    },
    {
      "path": "project3.html",
      "title": "Nonlinear modeling of lizard physiological data",
      "author": [],
      "contents": "\r\n\r\n\r\nhide\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\nlibrary(patchwork)\r\nlibrary(knitr)\r\nlibrary(broom)\r\n\r\n\r\n\r\nObjective:\r\nUsing the data from Lightfoot, D. and W.G. Whitford. 2020. Lizard pitfall trap data from 11 NPP study locations at the Jornada Basin LTER site, 1989-2006 ver 37. Environmental Data Initiative. https://doi.org/10.6073/pasta/4a6e258fb49c31e222ecbbcfd128967f\r\nData is measurement of weight and snout to vent length for various lizard species, divided by sex.\r\nWe will fit a model of weight as a function of snout to vent length of the form w = a*svl^b\r\n\r\n\r\nhide\r\n\r\nt2data = read.csv(\"lizard.csv\")\r\n\r\n\r\n\r\nFirst, let’s visualize the data by plotting weight against snouht to vent length, since we ultimately will be creating a function that does that.\r\n\r\n\r\nhide\r\n\r\nplot1 = ggplot(data=t2data,mapping=aes(x=SV_length,y=weight))+geom_point()+theme_classic()+labs(x=\"Snout to vent length (mm)\",y=\"Weight (g)\")\r\n#don't adjust axis, easier to see what's going on at weights near zero if they are away from x axis\r\nplot1+plot_annotation(caption=\"Weight in grams vs snout-vent length in mm, for all sampled lizards.\\nData from Lightfoot, D. and W.G. Whitford. 2020. Lizard pitfall trap data from 11\\nNPP study locations at the Jornada Basin LTER site, 1989-2006 ver 37.\\nEnvironmental Data Initiative. \\nhttps://doi.org/10.6073/pasta/4a6e258fb49c31e222ecbbcfd128967f\")\r\n\r\n\r\n\r\n\r\nThe curve to the data is not super sharp, as a rough initial guess I would expect b to be between 1 and 2.\r\nBut let’s get better estimates for the parameters!\r\nFirst we log transform the data, and then fit a linear regression to the result.\r\n\r\n\r\nhide\r\n\r\n#log transform the data and add as new columns\r\nt2data = mutate(t2data,log_wt = log(weight,base=10))\r\nt2data = mutate(t2data,log_svl = log(SV_length,base=10))\r\n\r\n#fit linear regression for log data\r\nml= lm(log_wt~log_svl,data=t2data)\r\nsummary(ml)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = log_wt ~ log_svl, data = t2data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.50177 -0.08274  0.01366  0.09089  2.50953 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -3.68084    0.05051  -72.88   <2e-16 ***\r\nlog_svl      2.53712    0.03033   83.66   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.162 on 1984 degrees of freedom\r\nMultiple R-squared:  0.7792,    Adjusted R-squared:  0.779 \r\nF-statistic:  7000 on 1 and 1984 DF,  p-value: < 2.2e-16\r\n\r\nWe will use the coefficients from this linear model as initial estimates for coefficients a and b, and then use those starting estimates to generate our nonlinear model using the nls function.\r\n\r\n\r\nhide\r\n\r\n#set up our equation\r\neqn = function(svl,a,b){\r\n  w=a*svl^b\r\n  return(w)\r\n}\r\n\r\n#generate the model using estimates from the linear model on the log\r\nnls1 = nls(weight~eqn(SV_length,a,b),data=t2data,start=list(a=(10^ml$coefficients[1]),b=ml$coefficients[2]),trace=FALSE)\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\n#get an additional column of the fitted model predictons\r\nt2data = augment(nls1,data=t2data)\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\nnls1tidy = broom::tidy(nls1)\r\nkable(nls1tidy,caption=c(\"Summary of model terms\"))\r\n\r\n\r\nTable 1: Summary of model terms\r\nterm\r\nestimate\r\nstd.error\r\nstatistic\r\np.value\r\na\r\n0.0003411\r\n0.0000400\r\n8.538421\r\n0\r\nb\r\n2.4532066\r\n0.0269791\r\n90.930009\r\n0\r\n\r\nRemember that the model’s format is weight = a*svl^b.\r\nIn reality, b is slightly higher than 2, and this is compensated by the extremely low a value.\r\n\r\n\r\nhide\r\n\r\n#plot the fitted weight against the actual weight\r\nplot2 = ggplot(data = t2data,mapping=aes(x=SV_length,y=weight))+geom_point(aes(color=sex))+labs(y=\"Weight (g)\", x = \"Snout to vent length (mm)\")+geom_line(data=t2data,mapping=aes(x=SV_length,y=.fitted))+theme_classic()\r\nplot2+plot_annotation(caption=\"Observed and predicted weight as a function of lizard snout to vent length.\\nBlack line represents the model predictions while points represent observed data.\")\r\n\r\n\r\n\r\n\r\nThis model does a pretty good job of reproducing observations. But can we do better?\r\nNext, we will make a model using a subset of the data, for Cnemidophorus tigrisatus males only. First we filter for only our desired sex and species. After that the steps are similar to what we did before. We will also want to compare the models, so we calculate the root mean square error for each model as well.\r\n\r\n\r\nhide\r\n\r\nt2data2 = filter(t2data,sex==\"M\"&spp==\"CNTI\")\r\ncolnames(t2data2)=c(\"x\",\"spp\",\"sex\",\"SV_length\",\"weight\",\"log_wt\",\"log_svl\",\"Fitted General\",\"Residuals General\")\r\n\r\nml2= lm(log_wt~log_svl,data=t2data2)\r\n\r\n\r\n#generate the model using estimates from the linear model on the log\r\nnls2 = nls(weight~eqn(SV_length,a,b),data=t2data2,start=list(a=(10^ml2$coefficients[1]),b=ml2$coefficients[2]),trace=FALSE)\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\n#get an additional column of the fitted model predictons\r\nt2data2 = augment(nls2,data=t2data2)\r\ncolnames(t2data2)=c(\"x\",\"spp\",\"sex\",\"SV_length\",\"weight\",\"log_wt\",\"log_svl\",\"Fitted General\",\"Residuals General\",\"Fitted Specific\",\"Residuals Specific\")\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\n#rmse function\r\nrmse = function(x,y){\r\n  return(sqrt(mean((x-y)^2)))\r\n}\r\nrmse_model1 = rmse(t2data2$weight,t2data2$`Fitted General`)\r\nrmse_model2 = rmse(t2data2$weight,t2data2$`Fitted Specific`)\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\n#plot the fitted weight against the actual weight\r\nplot3 = ggplot(data = t2data2,mapping=aes(x=SV_length,y=`Fitted General`))+geom_line(aes(color=\"Fitted General\"))+geom_line(aes(color=\"Fitted Specific\",x=SV_length,y=`Fitted Specific`))+geom_point(aes(color=\"Observed\",x=SV_length,y=weight))+labs(y=\"Model predicted weight (g)\", x = \"Snout to vent length (mm)\")+theme_classic()\r\nplot3+plot_annotation(caption=\"Observed and predicted weights using both the general and sex/species specific models\\nRMSE for Fitted General: 3.56\\nRMSE for Fitted Specific: 3.35\\nThe preferred model for this data is the species/sex specific model, as it has lower RMSE.\\nHowever, this model was developed specific for male Cnemidophorus tigrisatus,\\nand may not perform as well as the general model for females or other species.\")\r\n\r\n\r\n\r\n\r\nThis model outperforms our general model for this subset of the data. We wouldn’t necessarily expect it to do better for a different species or sex, though. One could easily automate the generation of additional models for specific combinations of species and sex by iterating the process above over each combination of sex and species.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:26-07:00"
    },
    {
      "path": "project4.html",
      "title": "Fish Passage on the Willamette River, Oregon",
      "author": [],
      "contents": "\r\n\r\nIntroduction\r\n\r\n Image source: M.O. Stevens & Fcb981 (ed.). 2007. Accessed 03/13/2022 via Wikipedia. The image is in the public domain.\r\nThe Willamette River is located in western Oregon and flows through the city of Portland. Fish counts have been collected at the Sullivan project dam, located at Willamette Falls, for fish making use of ladder to bypass the dam. We will be looking at three species of interest: Coho, Jack Coho, and Steelhead.\r\nData (c) 2001-2022, Oregon Department of Fish & Wildlife. Willamette Falls (Sullivan Project) Adult Passage Visual Counts. Accessed 03/13/2022 via Google Drive.\r\nAdditional data can be downloaded via Columbia River DART; the organization providing the data varies depending on the selected location.\r\nThe dam and fish ladder are located near West Lynn, Oregon, south of Portland.\r\n\r\nImage (c) 2022 Google and TerraMetrics, accessed 03/13/2022 at  Google Maps.\r\n\r\n\r\nhide\r\n\r\nfishes = read.csv(\"willamette_fish_passage.csv\") #load the data\r\n\r\n#we are working with Coho, Jack.Coho, and Steelhead, so let's get rid of everything else\r\n#we can remove the NAs (replacing with 0) at the same time\r\n\r\nfishes2 = data.frame(replace_na(pull(fishes,Date),0),replace_na(pull(fishes,Coho),0), #it doesn't appear as though there are NAs in the date column, but for consistency's sake...\r\n                     replace_na(pull(fishes,Jack.Coho),0),replace_na(pull(fishes,Steelhead),0))\r\ncolnames(fishes2)=c(\"Date\",\"Coho\",\"Jack Coho\",\"Steelhead\")\r\n\r\nfishes2$Date = mdy(fishes2$Date)\r\nfishes = as_tsibble(fishes2,key=NULL,index=Date)\r\n\r\n\r\n\r\nData Visualization and Takeaways\r\nOver Time\r\nHere we present the time series data for each of the three species of interest.\r\n\r\n\r\nhide\r\n\r\n#plot Coho, Jack.Coho, and Steelhead\r\n\r\n#get the species as a column value\r\nfishes_plotting = melt(as.data.frame(fishes),id=1)\r\ncolnames(fishes_plotting)=c(\"Date\",\"Species\",\"Count\")\r\n\r\n\r\n\r\nfishplot1 = ggplot(fishes_plotting,aes(x=Date,y=Count,group=Species))+geom_point(aes(color=Species,shape=Species),size=1)+#need to make the 3 lines a bit bigger so they can be seen where they overlap\r\n  theme_minimal()+scale_shape_manual(values=c(0,4,3))\r\n  \r\n\r\nfishplot1+plot_annotation(caption=\"Figure 1: Count over time for Coho, Jack Coho, and Steelhead, from 2001 to 2010.\\nSpecies are differentiated using the same color and symbol as in other figures.\\nHowever, the size of each point has been made smaller in this figure to reduce clutter.\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nhide\r\n\r\nfishplot2=ggplot(fishes,aes(x=Date,y=Coho))+geom_point(aes(color=\"#F8766D\",shape=0))+theme_minimal()+\r\n  theme(legend.position=\"none\")+scale_shape_identity()+scale_color_identity()\r\nfishplot3=ggplot(fishes,aes(x=Date,y=`Jack Coho`))+geom_point(aes(color=\"#00BA38\",shape=4))+theme_minimal()+\r\n  theme(legend.position=\"none\")+scale_shape_identity()+scale_color_identity()\r\nfishplot4=ggplot(fishes,aes(x=Date,y=Steelhead))+geom_point(aes(color=\"#619CFF\",shape=3))+theme_minimal()+\r\n  theme(legend.position=\"none\")+scale_shape_identity()+scale_color_identity()\r\n\r\n(fishplot2/fishplot3/fishplot4) + plot_annotation(caption=\"Figure 2: Separate graphs of count over time for each of Coho, Jack Coho, and Steelhead, from 2001 to 2010.\\nSpecies are differentiated using the same color and symbol as in other figures.\\nHowever, the size of each point has been made smaller in this figure to reduce clutter.\")\r\n\r\n\r\n\r\n\r\nFish counts rise and fall in a cyclical pattern for all three species. The timing of peak fish counts is approximately the same for Coho and Jack Coho, and happens in the latter half of the year. Outside the relatively short peak observation time, Coho and Jack Coho are virtually absent from the fish ladder. Steelhead count pattern differs slightly; it peaks at approximately the middle of the year, and there is a much wider range around the peak where steelhead are still being observed, albeit at lower numbers.\r\nSeasonplots\r\nBy using a season plot we can more easily observe the variation in fish counts over the course of a year.\r\n\r\n\r\nhide\r\n\r\n#create 3 plots, 1 for each species\r\ncoho_se=gg_season(fishes,y=Coho)+theme_minimal()+theme(legend.position=\"none\")\r\njcoho_se = gg_season(fishes,y=`Jack Coho`)+theme_minimal()+theme(legend.position=\"none\")\r\nsth_se = gg_season(fishes,y=Steelhead)+theme_minimal()+theme(legend.position=\"bottom\") #we only need 1 legend since the colors for each year are the same on all 3 plots\r\n\r\n#plot them\r\ncoho_se/jcoho_se/sth_se+plot_annotation(caption=\"Figure 3: Season plots for the three species.\\nY axis indicates the number of individuals observed, and the color of the lines on each plot indicates the year.\")\r\n\r\n\r\n\r\n\r\nThe season plot bears out the conclusions from looking at the time series data across all years. Coho and Jack Coho peak in October (late in the year) and are more ore less absent outside the August-December range. Conversely, Steelhead are observed throughout the year, with the peak observation time being in May-June. The season plot makes it easier to see that, for Steelhead, August-December counts are much lower than the counts for January-July.\r\nAnnual Totals\r\nHere we consider the yearly total counts at the fish ladder for each species.\r\n\r\n\r\nhide\r\n\r\n#get just the year in a column\r\nfishes_yr = as_tibble(fishes) %>%\r\n  mutate(Date=year(Date))%>%\r\n  group_by(Date) %>%\r\n  summarise(Coho = sum(Coho),`Jack Coho` = sum(`Jack Coho`),Steelhead = sum(Steelhead))\r\ncolnames(fishes_yr)=c(\"Year\",\"Coho\",\"Jack Coho\",\"Steelhead\")\r\nfishes_yr$Year=as.character(fishes_yr$Year) #converting it to a character data type should force it to be treated as categorical by default\r\n\r\nfy2 = melt(as.data.frame(fishes_yr),id=1)\r\ncolnames(fy2)=c(\"Year\",\"Species\",\"Count\")\r\n\r\nfyplot = ggplot(fy2,aes(x=Year,y=Count,group=Species))+geom_point(aes(color=Species,shape=Species),size=1.5,stroke=2)+\r\n  theme_minimal()+scale_shape_manual(values=c(0,4,3))+labs(y=\"Total Annual Count\")\r\nfyplot+plot_annotation(caption=\"Figure 4: Total annual count by year, for each of the three species of interest.\\nSpecies are differentiated using the same color and symbol as in other figures.\")\r\n\r\n\r\n\r\n\r\nBecause Jack Coho counts are much lower than the other species, a separate graph makes it easier to see the pattern in annual counts for Jack Coho.\r\n\r\n\r\nhide\r\n\r\nfyplot2 = ggplot(fishes_yr,aes(x=Year,y=`Jack Coho`))+geom_point(aes(color=\"#00BA38\",shape=4,size=1.5,stroke=2))+\r\n  theme_minimal()+labs(y=\"Jack Coho Count\")+theme(legend.position=\"none\")+scale_shape_identity()+scale_color_identity()+scale_size_identity()\r\nfyplot2+plot_annotation(caption=\"Figure 5: Total annual count by year for Jack Coho.\\nSpecies are differentiated using the same color and symbol as in other figures.\")\r\n\r\n\r\n\r\n\r\nFrom these figures, we observe:\r\nSteelhead annual counts are high (25000-50000) early in the 2000s, but decline to 20000-30000 from 2005 to 2009. The 2010 count for Steelhead is higher, at almost 35000; this may indicate that the population using the fish ladder is increasing again, but it is probably too soon to tell.\r\nCoho annual counts for most of the 2000s are relatively low compared to Coho counts in 2009 and 2010; values jump from a range of about 2000-10000 to over 20000 in the last two years of the dataset.\r\nJack Coho annual counts range from as low as 100 to about 3000, and this range does not seem to show an obvious increasing trend over the dataset.\r\nFor all three species, annual counts seem to rise and fall in an approximately 2-4 year cycle.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:34-07:00"
    },
    {
      "path": "Resume.html",
      "title": "My resume",
      "author": [],
      "contents": "\r\nClick this link to download a copy of my resume\r\n\r\nResume highlights:\r\n\r\n\r\nUniversity of California, Santa Barbara, 2017-Present\r\n\r\n\r\nI am a graduate student in the Geography Department. I completed my Masters degree in March 2020, and produced the thesis “\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:35-07:00"
    },
    {
      "path": "Resume2.html",
      "title": "My resume",
      "author": [],
      "contents": "\r\nClick this link to download a copy of my resume\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-14T15:48:35-07:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
